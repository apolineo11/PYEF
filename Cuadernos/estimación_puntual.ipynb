{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación Puntual de Parámetros\n",
    "\n",
    "## Introducción\n",
    "\n",
    "La estimación puntual es un procedimiento fundamental en estadística inferencial que permite obtener una única aproximación numérica de un parámetro desconocido de una población a partir de una muestra de datos. Su importancia radica en que proporciona una herramienta para realizar inferencias sobre la población sin necesidad de observarla en su totalidad, facilitando la toma de decisiones en contextos científicos, económicos e industriales. La calidad de un estimador puntual depende de sus propiedades, como insesgamiento, consistencia, eficiencia y suficiencia, las cuales garantizan que las estimaciones sean lo más precisas y confiables posible.\n",
    "\n",
    "---\n",
    "\n",
    "## Propiedades de los Estimadores\n",
    "\n",
    "### **1. Insesgamiento**\n",
    "Un estimador $\\hat{\\theta}$ es **insesgado** si su valor esperado es igual al parámetro:\n",
    "\n",
    "$$ E[\\hat{\\theta}] = \\theta $$\n",
    "\n",
    "Si un estimador no es insesgado, se dice que tiene **sesgo**, definido como:\n",
    "\n",
    "$$ B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta $$\n",
    "\n",
    "Si $B(\\hat{\\theta}) \\to 0$ cuando $n \\to \\infty$, el estimador es **asintóticamente insesgado**.\n",
    "\n",
    "### **2. Consistencia**\n",
    "Un estimador es **consistente** si converge en probabilidad al verdadero valor del parámetro a medida que el tamaño de la muestra aumenta:\n",
    "\n",
    "$$ \\hat{\\theta}_n \\to \\theta \\quad \\text{cuando} \\quad n \\to \\infty $$\n",
    "\n",
    "Equivale a decir que para cualquier $\\epsilon > 0$:\n",
    "\n",
    "$$ P(|\\hat{\\theta}_n - \\theta| < \\epsilon) \\to 1 \\quad \\text{cuando} \\quad n \\to \\infty $$\n",
    "\n",
    "### **3. Eficiencia**\n",
    "Un estimador es **eficiente** si tiene la menor varianza posible entre todos los estimadores insesgados del parámetro. Formalmente, si $\\hat{\\theta}_1$ y $\\hat{\\theta}_2$ son dos estimadores insesgados de $\\theta$, entonces $\\hat{\\theta}_1$ es más eficiente si:\n",
    "\n",
    "$$ \\text{Var}(\\hat{\\theta}_1) \\leq \\text{Var}(\\hat{\\theta}_2) \\quad \\forall \\theta $$\n",
    "\n",
    "Un estimador eficiente alcanza la **cota de Cramér-Rao**, que establece un límite inferior para la varianza de cualquier estimador insesgado:\n",
    "\n",
    "$$ \\text{Var}(\\hat{\\theta}) \\geq \\frac{1}{n I(\\theta)} $$\n",
    "\n",
    "donde $I(\\theta)$ es la **información de Fisher**.\n",
    "\n",
    "### **4. Suficiencia**\n",
    "Un estimador es **suficiente** si retiene toda la información relevante sobre el parámetro contenida en la muestra. Formalmente, una estadística $T(X)$ es suficiente para $\\theta$ si la distribución condicional de la muestra dado $T(X)$ no depende de $\\theta$:\n",
    "\n",
    "$$ f(X_1, ..., X_n \\mid T(X), \\theta) = f(X_1, ..., X_n \\mid T(X)) $$\n",
    "\n",
    "El **teorema de factorización de Fisher-Neyman** proporciona un criterio práctico para determinar si un estimador es suficiente.\n",
    "\n",
    "### **5. Robustez**\n",
    "Un estimador es **robusto** si es relativamente insensible a violaciones de los supuestos del modelo estadístico subyacente, como la presencia de valores atípicos o desviaciones de la normalidad en los datos. Matemáticamente, la robustez puede evaluarse a través de la influencia relativa de una observación en el estimador, lo que se formaliza con la **función de influencia**:\n",
    "\n",
    "$$ IF(X; T) = \\lim_{\\epsilon \\to 0} \\frac{T(F_\\epsilon) - T(F)}{\\epsilon} $$\n",
    "\n",
    "donde $T(F)$ es el estimador basado en la distribución $F$, y $F_\\epsilon$ es la distribución contaminada con una pequeña proporción $\\epsilon$ de valores atípicos. Un estimador robusto tiene una función de influencia acotada.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Métodos de Estimación\n",
    "\n",
    "### **1. Método de Máxima Verosimilitud (MLE)**\n",
    "\n",
    "El método de máxima verosimilitud es una técnica ampliamente utilizada para estimar parámetros en modelos estadísticos. Su principio fundamental es encontrar el valor del parámetro que maximiza la probabilidad de observar los datos dados.\n",
    "\n",
    "Dado un conjunto de observaciones $X_1, X_2, ..., X_n$, la función de verosimilitud se define como:\n",
    "\n",
    "$$ L(\\theta) = \\prod_{i=1}^{n} f(X_i \\mid \\theta) $$\n",
    "\n",
    "La estimación por máxima verosimilitud se obtiene maximizando la función de log-verosimilitud:\n",
    "\n",
    "$$ \\ell(\\theta) = \\sum_{i=1}^{n} \\log f(X_i \\mid \\theta) $$\n",
    "\n",
    "#### **Ejemplo: Estimación de los parámetros de una distribución normal**\n",
    "Supongamos que tenemos una muestra aleatoria de una distribución normal $N(\\mu, \\sigma^2)$. La función de verosimilitud es:\n",
    "\n",
    "$$ L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "Maximizando esta función, los estimadores de máxima verosimilitud son:\n",
    "\n",
    "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\hat{\\mu})^2 $$\n",
    "\n",
    "### **2. Método de los Momentos**\n",
    "\n",
    "El método de los momentos consiste en igualar los momentos teóricos de la distribución con los momentos muestrales y resolver para obtener los valores estimados de los parámetros.\n",
    "\n",
    "#### **Ejemplo: Estimación del parámetro de una distribución exponencial**\n",
    "\n",
    "Si una variable aleatoria $X$ sigue una distribución exponencial con parámetro $\\lambda$, su esperanza es:\n",
    "\n",
    "$$ E[X] = \\frac{1}{\\lambda} $$\n",
    "\n",
    "Igualando esto con la media muestral:\n",
    "\n",
    "$$ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\n",
    "\n",
    "El estimador de $\\lambda$ por momentos es:\n",
    "\n",
    "$$ \\hat{\\lambda} = \\frac{1}{\\bar{X}} $$\n",
    "\n",
    "### **3. Método por Analogía**\n",
    "\n",
    "El método por analogía selecciona un estimador que cumpla un papel análogo en la muestra al desempeñado por el parámetro en la población.\n",
    "\n",
    "#### **Ejemplo: Estimación del parámetro de una distribución uniforme**\n",
    "\n",
    "Para una variable aleatoria $X$ que sigue una distribución uniforme $U(0, \\theta)$, el máximo valor en la muestra es un estimador natural del parámetro superior $\\theta$. Es decir,\n",
    "\n",
    "$$ \\hat{\\theta} = \\max(X_1, X_2, ..., X_n) $$\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "Mayorga (2004) Inferencia estadística, Unal.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
